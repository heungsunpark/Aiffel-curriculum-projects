{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "194dfbee",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 다운로드\n",
    "- 이미 실습(1) 데이터 다듬기에서 Cloud shell에 심볼릭 링크로 ~/aiffel/lyricist/data를 생성하셨다면\n",
    "- ~/aiffel/lyricist/data/lyrics에 데이터가 있습니다.\n",
    "\n",
    "## Step 2. 데이터 읽어오기\n",
    "- glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요\n",
    "- glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할께요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db67c252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트 파일 갯수: 49\n",
      "데이터 크기: 187088\n",
      "첫 번째 문장: Now I've heard there was a secret chord\n",
      "센텐스 크기: 39\n",
      "단어 수: 8\n"
     ]
    }
   ],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# glob 를 활용하여 모든 txt 파일을 읽어오기 = txt_file_list\n",
    "\n",
    "file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "txt_file_list = glob(file_path)   \n",
    "\n",
    "print(\"텍스트 파일 갯수:\", len(txt_file_list))\n",
    "# print(txt_file_list)\n",
    "\n",
    "# raw_corpus 리스트에 문장 단위로 저장\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "\n",
    "for txt_file in txt_file_list:\n",
    "    \n",
    "    # 파일을 읽기모드로 열고\n",
    "    # 라인 단위로 끊어서 list 형태로 읽어옵니다.\n",
    "\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "# 앞에서부터 10라인, 뒤에서부터 10라인만 화면에 출력해 볼까요?\n",
    "# print(raw_corpus[:9])\n",
    "# print(raw_corpus[-10:])\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"첫 번째 문장:\", raw_corpus[0])\n",
    "print(\"센텐스 크기:\", len(raw_corpus[0]))\n",
    "print(\"단어 수:\", len(raw_corpus[0].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa0096",
   "metadata": {},
   "source": [
    "## Step 3. 데이터 정제\n",
    "- 앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "- preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd74dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "# 입력된 문장을\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "# 이 순서로 처리해주면 문제가 되는 상황을 방지할 수 있겠네요!\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7e649",
   "metadata": {},
   "source": [
    "- 추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다\n",
    "  - 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠\n",
    "  - 그래서 이번에는 문장을 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외하기를 권합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e4ff972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제된 데이터 크기: 168357\n"
     ]
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모을겁니다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "        \n",
    "    #토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외\n",
    "    if len(sentence.split()) > 15: continue\n",
    "        \n",
    "    # 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "    \n",
    "print(\"정제된 데이터 크기:\", len(corpus))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792eee1d",
   "metadata": {},
   "source": [
    "이제 데이터는 완벽하게 준비가 된 것 같네요!\n",
    "\n",
    "## Step 4. 평가 데이터셋 분리\n",
    "- tokenize() 함수로 데이터를 Tensor로 변환한 후\n",
    "- 단어장의 크기는 12,000 이상 으로 설정하세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e29c4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2706 ...    0    0    0]\n",
      " [   2   34    7 ...   44    3    0]\n",
      " ...\n",
      " [   2  259  194 ...   12    3    0]\n",
      " [   5   22    9 ...   10 1099    3]\n",
      " [   2    7   33 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f90881e04c0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0edd8138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168357, 15)\n"
     ]
    }
   ],
   "source": [
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9785e",
   "metadata": {},
   "source": [
    "- 단어 사전이 어떻게 구축되었는지 아래와 같이 확인해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a46f505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk> 2 : <start> 3 : <end> 4 : , 5 : i 6 : the 7 : you 8 : and 9 : a 10 : to 26729\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx], end=\" \")\n",
    "    if idx >= 10: break\n",
    "print(len(tokenizer.index_word.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee78eef",
   "metadata": {},
   "source": [
    "### 생성된 텐서를 소스와 타겟으로 분리  \n",
    "- 이 과정도 텐서플로우가 제공하는 모듈을 사용할 것이니, 어떻게 사용하는지만 눈여겨 봐둡시다.\n",
    "\n",
    "- 훈련 데이터와 평가 데이터를 분리하세요!\n",
    "- sklearn 모듈의 train_test_split() 함수를 사용해 **훈련 데이터**와 **평가 데이터**를 분리하도록 하겠습니다\n",
    "- 총 데이터의 **20%**를 평가 데이터셋으로 사용해 주세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fdbac22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  307   62   57    9  957 5739    3    0    0    0]\n",
      "[  50    5   91  307   62   57    9  957 5739    3    0    0    0    0]\n",
      "Source Train: (134685, 14)\n",
      "Target Train: (134685, 14)\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa80837",
   "metadata": {},
   "source": [
    "corpus 내의 첫 번째 문장에 대해 생성된 소스와 타겟 문장을 확인해 보았습니다.\n",
    "- 예상대로 소스는 2([start])에서 시작해서 3([end]으로 끝난 후 0([pad])로 채워져 있습니다\n",
    "- 하지만 타겟은 2로 시작하지 않고 소스를 왼쪽으로 한 칸 시프트 한 형태를 가지고 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d05252",
   "metadata": {},
   "source": [
    "### 데이터셋 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75748c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUFFER_SIZE = len(src_input)\n",
    "# BATCH_SIZE = 256\n",
    "# steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 120001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "# dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5e7c3f",
   "metadata": {},
   "source": [
    "### 모델 구성하기\n",
    "- tf.keras.Model을 Subclassing하는 방식\n",
    "- 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성\n",
    "- Embedding Layer의 ***embedding_size***=워드 벡터의 차원수, 즉 단어가 추상적으로 표현되는 크기\n",
    "  - 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만\n",
    "  - 그만큼 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기\n",
    "  - 이번 실습에서는 256이 적당해 보인다고...\n",
    "- LSTM 레이어의 hidden state의 차원 수인 ***hidden_size***도 같은 맥락\n",
    "  - hidden_size는 모델에 얼마나 많은 일꾼을 둘 것인가?로 이해해도 크게 엇나가지 않음\n",
    "  - 그 일꾼들은 모두 같은 데이터를 보고 각자의 생각을 가지는데\n",
    "  - 역시 충분한 데이터가 주어지면 올바른 결정을 내리겠지만 그렇지 않으면 배가 산으로 갈 뿐\n",
    "  - 이번 실습에는 1024가 적당해 보이는군요..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d94baf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206549e9",
   "metadata": {},
   "source": [
    "#### (for Step 6) 아래 generate_text 함수는 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문을 진행하게 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a5711b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fef60a",
   "metadata": {},
   "source": [
    "  \n",
    "## Step 5-1. 인공지능 만들기\n",
    "- 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n",
    "- (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "    - loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "- 그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca66ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aadb571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fdd7cb",
   "metadata": {},
   "source": [
    "- 이제 모델이 학습할 준비가 완료되었습니다. \n",
    "- 아래 코드를 실행해 모델을 학습시켜보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b34d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "526/526 [==============================] - 96s 172ms/step - loss: 3.6285 - val_loss: 3.2538\n",
      "Epoch 2/10\n",
      "526/526 [==============================] - 99s 189ms/step - loss: 3.1459 - val_loss: 3.0635\n",
      "Epoch 3/10\n",
      "526/526 [==============================] - 100s 191ms/step - loss: 2.9693 - val_loss: 2.9396\n",
      "Epoch 4/10\n",
      "526/526 [==============================] - 100s 191ms/step - loss: 2.8340 - val_loss: 2.8470\n",
      "Epoch 5/10\n",
      "526/526 [==============================] - 101s 191ms/step - loss: 2.7180 - val_loss: 2.7785\n",
      "Epoch 6/10\n",
      "526/526 [==============================] - 100s 190ms/step - loss: 2.6114 - val_loss: 2.7248\n",
      "Epoch 7/10\n",
      "526/526 [==============================] - 101s 191ms/step - loss: 2.5114 - val_loss: 2.6746\n",
      "Epoch 8/10\n",
      "526/526 [==============================] - 101s 191ms/step - loss: 2.4155 - val_loss: 2.6341\n",
      "Epoch 9/10\n",
      "526/526 [==============================] - 100s 191ms/step - loss: 2.3245 - val_loss: 2.5988\n",
      "Epoch 10/10\n",
      "526/526 [==============================] - 101s 191ms/step - loss: 2.2378 - val_loss: 2.5711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ec0039df0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa029b",
   "metadata": {},
   "source": [
    "## Step 6-1. 작문 모델 평가 - 작문 시켜보고 직접 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be89417a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작문 모델 평가 - 작문 시켜보고 직접 평가\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a70720",
   "metadata": {},
   "source": [
    "  \n",
    "## Step 5-2. 인공지능 만들기: hyperparameter 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edda7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256*2\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd72779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "526/526 [==============================] - 109s 200ms/step - loss: 3.5828 - val_loss: 3.2055\n",
      "Epoch 2/10\n",
      "526/526 [==============================] - 105s 199ms/step - loss: 3.0911 - val_loss: 3.0076\n",
      "Epoch 3/10\n",
      "526/526 [==============================] - 105s 199ms/step - loss: 2.9091 - val_loss: 2.8863\n",
      "Epoch 4/10\n",
      "526/526 [==============================] - 105s 199ms/step - loss: 2.7658 - val_loss: 2.7943\n",
      "Epoch 5/10\n",
      "526/526 [==============================] - 105s 200ms/step - loss: 2.6397 - val_loss: 2.7219\n",
      "Epoch 6/10\n",
      "526/526 [==============================] - 105s 200ms/step - loss: 2.5223 - val_loss: 2.6643\n",
      "Epoch 7/10\n",
      "526/526 [==============================] - 105s 199ms/step - loss: 2.4124 - val_loss: 2.6137\n",
      "Epoch 8/10\n",
      "526/526 [==============================] - 105s 200ms/step - loss: 2.3068 - val_loss: 2.5736\n",
      "Epoch 9/10\n",
      "526/526 [==============================] - 105s 200ms/step - loss: 2.2062 - val_loss: 2.5395\n",
      "Epoch 10/10\n",
      "526/526 [==============================] - 105s 200ms/step - loss: 2.1091 - val_loss: 2.5119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ec012e730>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d312c",
   "metadata": {},
   "source": [
    "## Step 6-2. 작문 모델 평가 - 작문 시켜보고 직접 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "522858c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작문 모델 평가 - 작문 시켜보고 직접 평가\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedc9a3a",
   "metadata": {},
   "source": [
    "  \n",
    "## Step 5-3. 인공지능 만들기: hyperparameter 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2130029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 1024*2\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b466d49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "526/526 [==============================] - 293s 523ms/step - loss: 3.5300 - val_loss: 3.1063\n",
      "Epoch 2/10\n",
      "526/526 [==============================] - 276s 524ms/step - loss: 2.9534 - val_loss: 2.8499\n",
      "Epoch 3/10\n",
      "526/526 [==============================] - 276s 524ms/step - loss: 2.6775 - val_loss: 2.6765\n",
      "Epoch 4/10\n",
      "526/526 [==============================] - 276s 524ms/step - loss: 2.4150 - val_loss: 2.5512\n",
      "Epoch 5/10\n",
      "526/526 [==============================] - 276s 524ms/step - loss: 2.1591 - val_loss: 2.4494\n",
      "Epoch 6/10\n",
      "526/526 [==============================] - 276s 525ms/step - loss: 1.9180 - val_loss: 2.3724\n",
      "Epoch 7/10\n",
      "526/526 [==============================] - 274s 521ms/step - loss: 1.6964 - val_loss: 2.3193\n",
      "Epoch 8/10\n",
      "526/526 [==============================] - 270s 514ms/step - loss: 1.5004 - val_loss: 2.2826\n",
      "Epoch 9/10\n",
      "526/526 [==============================] - 270s 513ms/step - loss: 1.3338 - val_loss: 2.2714\n",
      "Epoch 10/10\n",
      "526/526 [==============================] - 270s 514ms/step - loss: 1.1970 - val_loss: 2.2700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7e3b14f2b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f1abc5",
   "metadata": {},
   "source": [
    "## Step 6-3. 작문 모델 평가 - 작문 시켜보고 직접 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e474e756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you lie <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작문 모델 평가 - 작문 시켜보고 직접 평가\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ec4fa7",
   "metadata": {},
   "source": [
    "  \n",
    "## Step 5-4. 인공지능 만들기: hyperparameter 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8df7f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256*2\n",
    "hidden_size = 1024*2\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c41b2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "526/526 [==============================] - 340s 575ms/step - loss: 3.4597 - val_loss: 3.0629\n",
      "Epoch 2/20\n",
      "526/526 [==============================] - 308s 586ms/step - loss: 2.8963 - val_loss: 2.7926\n",
      "Epoch 3/20\n",
      "526/526 [==============================] - 308s 586ms/step - loss: 2.5955 - val_loss: 2.6158\n",
      "Epoch 4/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 2.3119 - val_loss: 2.4833\n",
      "Epoch 5/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 2.0380 - val_loss: 2.3863\n",
      "Epoch 6/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 1.7823 - val_loss: 2.3149\n",
      "Epoch 7/20\n",
      "526/526 [==============================] - 309s 588ms/step - loss: 1.5520 - val_loss: 2.2686\n",
      "Epoch 8/20\n",
      "526/526 [==============================] - 349s 664ms/step - loss: 1.3551 - val_loss: 2.2455\n",
      "Epoch 9/20\n",
      "526/526 [==============================] - 310s 589ms/step - loss: 1.1959 - val_loss: 2.2459\n",
      "Epoch 10/20\n",
      "526/526 [==============================] - 309s 588ms/step - loss: 1.0808 - val_loss: 2.2638\n",
      "Epoch 11/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 1.0071 - val_loss: 2.2845\n",
      "Epoch 12/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 0.9650 - val_loss: 2.3131\n",
      "Epoch 13/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 0.9403 - val_loss: 2.3304\n",
      "Epoch 14/20\n",
      "526/526 [==============================] - 309s 588ms/step - loss: 0.9248 - val_loss: 2.3509\n",
      "Epoch 15/20\n",
      "526/526 [==============================] - 309s 588ms/step - loss: 0.9145 - val_loss: 2.3686\n",
      "Epoch 16/20\n",
      "526/526 [==============================] - 309s 588ms/step - loss: 0.9067 - val_loss: 2.3835\n",
      "Epoch 17/20\n",
      "526/526 [==============================] - 309s 587ms/step - loss: 0.9008 - val_loss: 2.3979\n",
      "Epoch 18/20\n",
      "526/526 [==============================] - 309s 588ms/step - loss: 0.8952 - val_loss: 2.4070\n",
      "Epoch 19/20\n",
      "526/526 [==============================] - 310s 589ms/step - loss: 0.8900 - val_loss: 2.4237\n",
      "Epoch 20/20\n",
      "526/526 [==============================] - 310s 589ms/step - loss: 0.8865 - val_loss: 2.4364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f90500fa040>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=20, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39803e7",
   "metadata": {},
   "source": [
    "## Step 6-4. 작문 모델 평가 - 작문 시켜보고 직접 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5726b291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you more than you ll ever know <end> '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작문 모델 평가 - 작문 시켜보고 직접 평가\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe7e70",
   "metadata": {},
   "source": [
    "  \n",
    "## Step 5-5. 인공지능 만들기: hyperparameter 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10805d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256*3\n",
    "hidden_size = 1024*3\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb210873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "526/526 [==============================] - 674s 1s/step - loss: 3.4119 - val_loss: 3.0461\n",
      "Epoch 2/10\n",
      "526/526 [==============================] - 623s 1s/step - loss: 2.8944 - val_loss: 2.8035\n",
      "Epoch 3/10\n",
      "526/526 [==============================] - 623s 1s/step - loss: 2.6237 - val_loss: 2.6467\n",
      "Epoch 4/10\n",
      "526/526 [==============================] - 624s 1s/step - loss: 2.3709 - val_loss: 2.5262\n",
      "Epoch 5/10\n",
      "526/526 [==============================] - 624s 1s/step - loss: 2.1255 - val_loss: 2.4323\n",
      "Epoch 6/10\n",
      "526/526 [==============================] - 624s 1s/step - loss: 1.8901 - val_loss: 2.3601\n",
      "Epoch 7/10\n",
      "526/526 [==============================] - 624s 1s/step - loss: 1.6677 - val_loss: 2.3056\n",
      "Epoch 8/10\n",
      "526/526 [==============================] - 625s 1s/step - loss: 1.4652 - val_loss: 2.2738\n",
      "Epoch 9/10\n",
      "526/526 [==============================] - 624s 1s/step - loss: 1.2877 - val_loss: 2.2531\n",
      "Epoch 10/10\n",
      "526/526 [==============================] - 624s 1s/step - loss: 1.1422 - val_loss: 2.2608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fd1f979d0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623ba41e",
   "metadata": {},
   "source": [
    "## Step 6-5. 작문 모델 평가 - 작문 시켜보고 직접 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa150d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you , i love you <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작문 모델 평가 - 작문 시켜보고 직접 평가\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1b7a5",
   "metadata": {},
   "source": [
    "  \n",
    "## Step 5-6. 인공지능 만들기: hyperparameter 변경 (5-4와 같으며 epoch 조정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baf7f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256*2\n",
    "hidden_size = 1024*2\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af1917e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "526/526 [==============================] - 330s 592ms/step - loss: 3.4636 - val_loss: 3.0442\n",
      "Epoch 2/8\n",
      "526/526 [==============================] - 310s 590ms/step - loss: 2.8874 - val_loss: 2.7828\n",
      "Epoch 3/8\n",
      "526/526 [==============================] - 311s 591ms/step - loss: 2.5817 - val_loss: 2.5994\n",
      "Epoch 4/8\n",
      "526/526 [==============================] - 311s 591ms/step - loss: 2.2869 - val_loss: 2.4649\n",
      "Epoch 5/8\n",
      "526/526 [==============================] - 311s 591ms/step - loss: 2.0038 - val_loss: 2.3644\n",
      "Epoch 6/8\n",
      "526/526 [==============================] - 311s 591ms/step - loss: 1.7406 - val_loss: 2.2914\n",
      "Epoch 7/8\n",
      "526/526 [==============================] - 311s 592ms/step - loss: 1.5074 - val_loss: 2.2472\n",
      "Epoch 8/8\n",
      "526/526 [==============================] - 312s 592ms/step - loss: 1.3108 - val_loss: 2.2292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8fd114d2b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optimizer와 loss등은 차차 배웁니다\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=8, validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840fc250",
   "metadata": {},
   "source": [
    "## Step 6-6. 작문 모델 평가 - 작문 시켜보고 직접 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f21099ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 작문 모델 평가 - 작문 시켜보고 직접 평가\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97df03",
   "metadata": {},
   "source": [
    "# 4-7. 프로젝트: 멋진 작사가 만들기 \n",
    "## Step 1. 데이터 다운로드\n",
    "[문제]\n",
    "- 이미 실습(1) 데이터 다듬기에서 Cloud shell에 심볼릭 링크로 ~/aiffel/lyricist/data를 생성하셨다면, ~/aiffel/lyricist/data/lyrics에 데이터가 있습니다.  \n",
    "\n",
    "[실행]\n",
    "- 실행\n",
    "\n",
    "## Step 2. 데이터 읽어오기\n",
    "[문제]\n",
    "- glob 모듈을 사용하면 파일을 읽어오는 작업을 하기가 아주 용이해요.\n",
    "- glob 를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장하도록 할게요!  \n",
    "\n",
    "[실행]\n",
    "- glob 함수를 처음 보는 것이어서 웹 검색하여 해결\n",
    "- glob 함수를 써서 읽어온 텍스트 파일의 리스트 생성\n",
    "- 여러 개의 파일을 읽어서 합쳐야 하므로 텍스트 파일 리스트에 대해 for문 사용\n",
    " \n",
    "## Step 3. 데이터 정제\n",
    "[문제]\n",
    "- 앞서 배운 테크닉들을 활용해 문장 생성에 적합한 모양새로 데이터를 정제하세요!\n",
    "- preprocess_sentence() 함수를 만든 것을 기억하시죠? 이를 활용해 데이터를 정제하도록 하겠습니다.\n",
    "- 추가로 지나치게 긴 문장은 다른 데이터들이 과도한 Padding을 갖게 하므로 제거합니다.\n",
    "  - 너무 긴 문장은 노래 가사 작사하기에 어울리지 않을 수도 있겠죠.\n",
    "  - 그래서 이번에는 문장을 토큰화 했을 때 **토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외**하기를 권합니다.  \n",
    "  \n",
    "[실행]\n",
    "- 토큰 갯수가 15개 초과하는 문장을 제외할 때 \"if len(sentence.split()) > 15: continue\"를 사용\n",
    "- sentence.split()를 못 찾아서 애먹었음 ㅎ~ 파이썬이 능숙치 못해 힘듬 ㅠㅠ\n",
    "\n",
    "## Step 4. 평가 데이터셋 분리\n",
    "[문제]\n",
    "- 훈련 데이터와 평가 데이터를 분리하세요!\n",
    "  - tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다.\n",
    "- 단어장의 크기는 12,000 이상으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!  \n",
    "\n",
    "[실행]\n",
    "- 데이터 분리는 큰 문제 없이 해결\n",
    "  - train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 42) 사용\n",
    "- 단어장 크기는 tokenize() 함수의 num_words (=7000) 파라미터 값 변경: 7000 -> 12000\n",
    "\n",
    "[문제]\n",
    "- Source Train: (124960, 14)\n",
    "- Target Train: (124960, 14)\n",
    "  - 만약 결과가 다르다면 천천히 과정을 다시 살펴 동일한 결과를 얻도록 하세요!\n",
    "  - 만약 학습 데이터 개수가 124960보다 크다면 위 Step 3.의 데이터 정제 과정을 다시 한번 검토해 보시기를 권합니다.  \n",
    "  \n",
    "[실행]\n",
    "- Source Train: (134685, 14)\n",
    "- Target Train: (134685, 14)\n",
    "  - 교재의 결과와 다르게 나와 데이터 정제 과정을 여러 번 검토하였으나 특이사항 발견 못함... (pass^^)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245266eb",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5. 인공지능 만들기\n",
    "[문제] \n",
    "- 모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 **val_loss 값을 2.2 수준으로** 줄일 수 있는 모델을 설계하세요! \n",
    "  - (Loss는 아래 제시된 Loss 함수를 그대로 사용!)\n",
    "- 그리고 멋진 모델이 생성한 가사 한 줄을 제출하시길 바랍니다!  \n",
    "\n",
    "[실행] \n",
    "- 아래와 같은 여섯 가지 경우를 실행:\n",
    "\n",
    "\n",
    "|  Case | Embedding Size | Hidden Size | Epoch | val_loss | Remark | Lyrics generated |  \n",
    "|-----|:-----:|:-----:|:-----:|:-----:|:----- |:----- |    \n",
    "| 5-1 | 256 | 1024 | 10 | 2.5711 | Epoch 1->10까지 val_loss 지속 감소 | i love you |  \n",
    "| 5-2 | 512 | 1024 | 10 | 2.5119 | Epoch 1->10까지 val_loss 지속 감소 | i love you, i love you |  \n",
    "| 5-3 | 256 | 2048 | 10 | 2.2700 | Epoch 1->10까지 val_loss 지속 감소 | i love the way you lie |  \n",
    "| 5-4 | 512 | 2048 | 20 | 2.4364 | Epoch 8에서 val_loss 최소값 2.2455 기록 후 다시 증가. loss 값은 계속 감소... overfitting | i love you more than you ll ever know |  \n",
    "| 5-5 | 768 | 3072 | 10 | 2.2608 | Epoch 1->10까지 val_loss 지속 감소 | i love you, i love you, i love you |  \n",
    "| **5-6** | **512** | **2048** | **8** | **2.2292** | Case 5-4의 Epoch=8 val_loss값이 2.2 수준의 최소이므로 5-4를, overfitting이 일어나지 않도록 Epoch수를 줄여서 재실행 | i love you |\n",
    "  \n",
    "- Embedding Size와 Hidden Size를 번갈아 증가시켜 가면서 val_loss값 변화 추이 관찰\n",
    "- Embedding Size=512와 Hidden Size=2048(Case 5-4)까지는 이전 경우들보다 val_loss값 감소\n",
    "  - 이 경우 Epoch 8 이후로 val_loss값이 증가하면서 Overfitting이 발생함\n",
    "- Embedding Size=768과 Hidden Size=3072(Case 5-5)로 변화시켜 학습시킨 결과(val_loss 2.2608), 적은 값이긴 하나 Case 5-4 Epoch=8의 값(2.2455)보다 val_loss값 증가\n",
    "- Case 5-4의 Epoch=8에서 val_loss값이 2.2 수준이므로 Epoch수를 줄여서 Case 5-6을 만듦\n",
    "- Case 5-6을 학습시킨 결과 val_loss값이 2.2 수준(2.2292)이 되어 추가 모델 설계 종료\n",
    "- 이 멋진 모델이 만들어낸 가사 한 줄은... \n",
    "###  i love you\n",
    "  - 이 작사가는 미사려구보다는 직진인가요???\n",
    "\n",
    "## Comment\n",
    "- 교재에서 시키는 대로 따라하는 수준\n",
    "- 파라미터의 변경도 주먹구구식\n",
    "- 모델이 의미하는 바를 더 공부하고\n",
    "  - 학습단계에 따른 accuracy/loss 변화 그래프도 그려보아야 하나 몰라서 못함\n",
    "- 훈련데이터 분석도 보다 심도있게 해야\n",
    "  - EDA를 제대로 해야 (문장별 단어 수 분포 등) 하나 몰라서 못함\n",
    "- 작사가 수준의 AI 모델을 개발할 수 있을 것으로 기대됨\n",
    "- (추가로...) 모델 학습시간이 많이(?) 소요되고 초보자의 입장에서는 배우는 양이 많아서 Exploration 과제에 많은 시간 투자도 어려움\n",
    "  - 시간을 더 투자하고 지식이 늘어난다면 더 잘 할 수 있을 듯..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
